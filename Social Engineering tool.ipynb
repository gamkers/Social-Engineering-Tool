{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63280540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import BeautifulSoup\n",
    "def pearson(my_string):\n",
    "    name = my_string\n",
    "    \n",
    "    print(\"analysing and collecting data about a person\")\n",
    "    print(\"i think this is enough to know about a person sir\")\n",
    "    headers = {\n",
    "        'Access-Control-Allow-Origin': '*',\n",
    "        'Access-Control-Allow-Methods': 'GET',\n",
    "        'Access-Control-Allow-Headers': 'Content-Type',\n",
    "        'Access-Control-Max-Age': '3600',\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'\n",
    "    }\n",
    "    try:\n",
    "        url = \"https://www.google.com/search?q=\" + name\n",
    "        response = requests.get(url, headers=headers)\n",
    "        socialmedia = [\"instagram\", \"facebook\", \"twitter\", \"linkedin\", \"github\", \"scholar\", \"hackerearth\",\n",
    "                       \"hackerrank\", \"hackerone\", \"tiktok\", \"youtube\", \"books\", \"researchgate\", \"publons\", \"orcid\",\n",
    "                       \"maps\"]\n",
    "        linklist = []\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for g in soup.find_all('div', class_='g'):\n",
    "            anchors = g.find_all('a')\n",
    "            if 'href' in str(anchors[0]):\n",
    "                linklist.append(anchors[0]['href'])\n",
    "        c = 0\n",
    "        foundedlinks = []\n",
    "        for i in socialmedia:\n",
    "            sm = str(i)\n",
    "\n",
    "            for j in linklist:\n",
    "                if sm in str(j):\n",
    "                    c = c + 1\n",
    "                    foundedlinks.append(j)\n",
    "                    print(\"[+]\" + j)\n",
    "\n",
    "        for i in foundedlinks:\n",
    "            webbrowser.open(i)\n",
    "        print(\"[-] Checking for any pdf documents associated with this name .....\")\n",
    "        url = \"https://www.google.com/search?q=%22\" + name + \"%22+filetype%3Apdf&oq=%22\" + name + \"%22+filetype%3Apdf\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        f = 0\n",
    "        for g in soup.find_all('div', class_='g'):\n",
    "            links = g.find_all('a')\n",
    "            if 'href' in str(links[0]):\n",
    "                print(\"[+]\" + links[0]['href'])\n",
    "        if c == 0:\n",
    "            print(\"No Info about this person\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "pearson(\"mark zuckerberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892dd5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9615138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c89000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
